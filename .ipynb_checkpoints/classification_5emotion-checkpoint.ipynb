{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3651bd-8c96-46ab-ac6e-8fdb6b15dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e4ae12-cc88-45ef-a9b4-57a93fde7a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527abc7d-0aa3-49f1-9964-dc80a8b6ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3909124e-814e-4a09-b7f2-ff3a5017026b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 23  23  23]\n",
      "  [ 52  52  52]\n",
      "  [ 54  54  54]\n",
      "  ...\n",
      "  [173 173 173]\n",
      "  [ 98  98  98]\n",
      "  [ 42  42  42]]\n",
      "\n",
      " [[ 23  23  23]\n",
      "  [ 48  48  48]\n",
      "  [ 53  53  53]\n",
      "  ...\n",
      "  [174 174 174]\n",
      "  [104 104 104]\n",
      "  [ 34  34  34]]\n",
      "\n",
      " [[ 30  30  30]\n",
      "  [ 50  50  50]\n",
      "  [ 55  55  55]\n",
      "  ...\n",
      "  [175 175 175]\n",
      "  [113 113 113]\n",
      "  [ 36  36  36]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 31  31  31]\n",
      "  [ 28  28  28]\n",
      "  [ 29  29  29]\n",
      "  ...\n",
      "  [ 54  54  54]\n",
      "  [ 80  80  80]\n",
      "  [101 101 101]]\n",
      "\n",
      " [[ 31  31  31]\n",
      "  [ 29  29  29]\n",
      "  [ 30  30  30]\n",
      "  ...\n",
      "  [ 55  55  55]\n",
      "  [ 80  80  80]\n",
      "  [ 96  96  96]]\n",
      "\n",
      " [[ 29  29  29]\n",
      "  [ 31  31  31]\n",
      "  [ 28  28  28]\n",
      "  ...\n",
      "  [ 59  59  59]\n",
      "  [ 79  79  79]\n",
      "  [ 89  89  89]]]\n"
     ]
    }
   ],
   "source": [
    "cv2_image = cv2.imread(\"./FER-2013/train/angry/Training_992349.jpg\")\n",
    "print(cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4ee0a7-9778-484a-99c8-032386edfb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c6bce3c8b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfc8f033-3279-4fee-a150-b4cd1c1d3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fcd7b3-bb11-4d33-88c5-80a57fe09e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95aa53a7-fc95-4fdf-a4b6-aa04f39c46be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    './FER-2013/train',\n",
    "    subset='training',\n",
    "    target_size = (48, 48),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ac4a62-6a92-4f15-bc85-092a5378fed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5741 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = train_data_gen.flow_from_directory(\n",
    "    './FER-2013/train',\n",
    "    subset='validation',\n",
    "    target_size = (48, 48),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25dd74ed-c15c-4223-b7a2-5e62ec6bb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    './FER-2013/test',\n",
    "    target_size = (48, 48),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fe1e029-1bf3-4704-96ae-0ecc4dab1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48, 48, 1), padding='same'))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45aa62d-c3e9-4ec7-8acc-33be9c086b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 46, 46, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 46, 46, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 23, 23, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 23, 23, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 23, 23, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 21, 21, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 21, 21, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 10, 10, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 10, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 10, 10, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 10, 10, 512)       2359808   \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 10, 10, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 5, 5, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 5, 5, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              13108224  \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 7175      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,046,535\n",
      "Trainable params: 17,044,871\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fe903de-8197-4b49-b9d4-4672ea1db527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001, decay=1e-6), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c3c95f-bef6-4f9a-aa5e-325bfab926d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='c://Projects/emotion_detect_project/best_model/', monitor='val_loss', mode='min', save_vest_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0a05127-7343-407a-be4c-434be83af13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "929d0a8d-b8c6-48fa-a2a5-d1240c6376dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_delta=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56815377-9cb6-4176-9092-2f6322fe81fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "714/717 [============================>.] - ETA: 0s - loss: 2.6303 - accuracy: 0.2349"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 24s 26ms/step - loss: 2.6271 - accuracy: 0.2351 - val_loss: 1.7691 - val_accuracy: 0.2512 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 1.7807 - accuracy: 0.2633"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.7807 - accuracy: 0.2633 - val_loss: 1.7609 - val_accuracy: 0.2792 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 1.7048 - accuracy: 0.3068"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.7047 - accuracy: 0.3069 - val_loss: 1.8175 - val_accuracy: 0.2771 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 1.6379 - accuracy: 0.3393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.6382 - accuracy: 0.3392 - val_loss: 1.5955 - val_accuracy: 0.3738 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 1.5815 - accuracy: 0.3722"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.5812 - accuracy: 0.3725 - val_loss: 1.5194 - val_accuracy: 0.3949 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 1.5232 - accuracy: 0.4004"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.5227 - accuracy: 0.4008 - val_loss: 1.3940 - val_accuracy: 0.4602 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 1.4719 - accuracy: 0.4207"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 24ms/step - loss: 1.4719 - accuracy: 0.4207 - val_loss: 1.7079 - val_accuracy: 0.3500 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 1.4480 - accuracy: 0.4302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.4480 - accuracy: 0.4302 - val_loss: 1.3734 - val_accuracy: 0.4791 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 1.4105 - accuracy: 0.4460"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.4105 - accuracy: 0.4460 - val_loss: 1.6068 - val_accuracy: 0.3670 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 1.3816 - accuracy: 0.4622"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.3816 - accuracy: 0.4622 - val_loss: 1.3417 - val_accuracy: 0.4907 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 1.3470 - accuracy: 0.4806"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.3470 - accuracy: 0.4806 - val_loss: 1.4880 - val_accuracy: 0.4621 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 1.3165 - accuracy: 0.4935"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.3165 - accuracy: 0.4935 - val_loss: 1.2768 - val_accuracy: 0.5161 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 1.2816 - accuracy: 0.5085"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.2819 - accuracy: 0.5082 - val_loss: 1.2460 - val_accuracy: 0.5290 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 1.2442 - accuracy: 0.5242"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.2442 - accuracy: 0.5242 - val_loss: 1.2479 - val_accuracy: 0.5414 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 1.2084 - accuracy: 0.5419"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.2080 - accuracy: 0.5421 - val_loss: 1.2329 - val_accuracy: 0.5316 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 1.1579 - accuracy: 0.5592"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.1579 - accuracy: 0.5592 - val_loss: 1.2081 - val_accuracy: 0.5524 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 1.1173 - accuracy: 0.5783"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.1173 - accuracy: 0.5783 - val_loss: 1.2478 - val_accuracy: 0.5585 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 1.0629 - accuracy: 0.5966"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 26ms/step - loss: 1.0629 - accuracy: 0.5966 - val_loss: 1.1976 - val_accuracy: 0.5656 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 1.0309 - accuracy: 0.6061"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 1.0312 - accuracy: 0.6062 - val_loss: 1.2865 - val_accuracy: 0.5491 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "714/717 [============================>.] - ETA: 0s - loss: 0.9988 - accuracy: 0.6183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.9986 - accuracy: 0.6185 - val_loss: 1.2092 - val_accuracy: 0.5669 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 0.9534 - accuracy: 0.6405"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.9534 - accuracy: 0.6405 - val_loss: 1.3633 - val_accuracy: 0.5805 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 0.9050 - accuracy: 0.6592"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.9051 - accuracy: 0.6594 - val_loss: 1.3293 - val_accuracy: 0.5779 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 0.8781 - accuracy: 0.6679"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.8781 - accuracy: 0.6679 - val_loss: 1.3352 - val_accuracy: 0.5850 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.8306 - accuracy: 0.6871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.8312 - accuracy: 0.6869 - val_loss: 1.4787 - val_accuracy: 0.5744 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 0.8004 - accuracy: 0.7023"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.8004 - accuracy: 0.7023 - val_loss: 1.3463 - val_accuracy: 0.5801 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "714/717 [============================>.] - ETA: 0s - loss: 0.7542 - accuracy: 0.7129"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.7549 - accuracy: 0.7126 - val_loss: 1.4749 - val_accuracy: 0.5766 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.7244 - accuracy: 0.7303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 17s 24ms/step - loss: 0.7244 - accuracy: 0.7303 - val_loss: 1.3542 - val_accuracy: 0.5934 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 0.6857 - accuracy: 0.7431"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.6856 - accuracy: 0.7431 - val_loss: 1.5314 - val_accuracy: 0.5834 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "714/717 [============================>.] - ETA: 0s - loss: 0.5830 - accuracy: 0.7797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.5831 - accuracy: 0.7798 - val_loss: 1.5118 - val_accuracy: 0.6020 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 0.5324 - accuracy: 0.7988"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.5321 - accuracy: 0.7989 - val_loss: 1.4806 - val_accuracy: 0.6100 - lr: 2.0000e-04\n",
      "Epoch 31/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.4913 - accuracy: 0.8178"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 17s 24ms/step - loss: 0.4914 - accuracy: 0.8178 - val_loss: 1.6274 - val_accuracy: 0.6102 - lr: 2.0000e-04\n",
      "Epoch 32/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.4684 - accuracy: 0.8272"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.4679 - accuracy: 0.8273 - val_loss: 1.6485 - val_accuracy: 0.6100 - lr: 2.0000e-04\n",
      "Epoch 33/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 0.4503 - accuracy: 0.8318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 24ms/step - loss: 0.4505 - accuracy: 0.8317 - val_loss: 1.6374 - val_accuracy: 0.6091 - lr: 2.0000e-04\n",
      "Epoch 34/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 0.4259 - accuracy: 0.8392"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.4259 - accuracy: 0.8392 - val_loss: 1.6802 - val_accuracy: 0.6114 - lr: 2.0000e-04\n",
      "Epoch 35/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 0.4057 - accuracy: 0.8475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.4057 - accuracy: 0.8474 - val_loss: 1.8434 - val_accuracy: 0.6152 - lr: 2.0000e-04\n",
      "Epoch 36/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.8563"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.3855 - accuracy: 0.8563 - val_loss: 1.8309 - val_accuracy: 0.6117 - lr: 2.0000e-04\n",
      "Epoch 37/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.8648"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 24ms/step - loss: 0.3698 - accuracy: 0.8648 - val_loss: 1.8822 - val_accuracy: 0.6081 - lr: 2.0000e-04\n",
      "Epoch 38/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.3445 - accuracy: 0.8693"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.3445 - accuracy: 0.8692 - val_loss: 1.9224 - val_accuracy: 0.6082 - lr: 2.0000e-04\n",
      "Epoch 39/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.8833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.3127 - accuracy: 0.8833 - val_loss: 1.9270 - val_accuracy: 0.6110 - lr: 4.0000e-05\n",
      "Epoch 40/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.8837"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.3132 - accuracy: 0.8836 - val_loss: 1.9610 - val_accuracy: 0.6161 - lr: 4.0000e-05\n",
      "Epoch 41/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 0.3125 - accuracy: 0.8833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.3124 - accuracy: 0.8833 - val_loss: 1.9969 - val_accuracy: 0.6128 - lr: 4.0000e-05\n",
      "Epoch 42/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.2925 - accuracy: 0.8917 - val_loss: 1.9908 - val_accuracy: 0.6126 - lr: 4.0000e-05\n",
      "Epoch 43/100\n",
      "716/717 [============================>.] - ETA: 0s - loss: 0.2906 - accuracy: 0.8935"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.2904 - accuracy: 0.8936 - val_loss: 2.0290 - val_accuracy: 0.6149 - lr: 4.0000e-05\n",
      "Epoch 44/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.2851 - accuracy: 0.8931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.2851 - accuracy: 0.8931 - val_loss: 2.0272 - val_accuracy: 0.6140 - lr: 4.0000e-05\n",
      "Epoch 45/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.2846 - accuracy: 0.8941"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.2842 - accuracy: 0.8942 - val_loss: 2.0745 - val_accuracy: 0.6105 - lr: 4.0000e-05\n",
      "Epoch 46/100\n",
      "717/717 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.8984"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.2731 - accuracy: 0.8984 - val_loss: 2.0612 - val_accuracy: 0.6133 - lr: 4.0000e-05\n",
      "Epoch 47/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.2692 - accuracy: 0.8982"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 18s 25ms/step - loss: 0.2691 - accuracy: 0.8982 - val_loss: 2.0844 - val_accuracy: 0.6133 - lr: 4.0000e-05\n",
      "Epoch 48/100\n",
      "715/717 [============================>.] - ETA: 0s - loss: 0.2658 - accuracy: 0.8990"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "717/717 [==============================] - 19s 26ms/step - loss: 0.2660 - accuracy: 0.8990 - val_loss: 2.1020 - val_accuracy: 0.6138 - lr: 4.0000e-05\n"
     ]
    }
   ],
   "source": [
    "emotion_model_info = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.n // BATCH_SIZE,\n",
    "    epochs = 100,\n",
    "    validation_data = valid_generator,\n",
    "    validation_steps = valid_generator.n // BATCH_SIZE,\n",
    "    callbacks=[checkpoint, earlystopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "055c69cf-82c1-4529-bfec-0a385d435c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 28s 255ms/step - loss: 1.1204 - accuracy: 0.5751\n",
      "accuracy: 57.51%\n",
      "loss: 1.12\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator, batch_size = BATCH_SIZE, steps= test_generator.n//BATCH_SIZE)\n",
    "print(f\"accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"loss: {test_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce5b1e69-eaed-4aa0-be4f-9f1876649dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 4s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2e67940-ea1a-4bbb-b093-1c76f747d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bca9f737-36d0-4ce0-a32f-a279ff1c7bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.12      0.10      0.11       958\n",
      "     disgust       0.03      0.02      0.02       111\n",
      "        fear       0.14      0.12      0.13      1024\n",
      "       happy       0.25      0.31      0.28      1774\n",
      "     neutral       0.18      0.21      0.19      1233\n",
      "         sad       0.17      0.13      0.15      1247\n",
      "    surprise       0.12      0.12      0.12       831\n",
      "\n",
      "    accuracy                           0.18      7178\n",
      "   macro avg       0.14      0.14      0.14      7178\n",
      "weighted avg       0.17      0.18      0.17      7178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa75ca1-61c7-4d8b-bb34-0d9eae63e3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
