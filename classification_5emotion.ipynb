{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3651bd-8c96-46ab-ac6e-8fdb6b15dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e4ae12-cc88-45ef-a9b4-57a93fde7a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "527abc7d-0aa3-49f1-9964-dc80a8b6ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3909124e-814e-4a09-b7f2-ff3a5017026b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 23  23  23]\n",
      "  [ 52  52  52]\n",
      "  [ 54  54  54]\n",
      "  ...\n",
      "  [173 173 173]\n",
      "  [ 98  98  98]\n",
      "  [ 42  42  42]]\n",
      "\n",
      " [[ 23  23  23]\n",
      "  [ 48  48  48]\n",
      "  [ 53  53  53]\n",
      "  ...\n",
      "  [174 174 174]\n",
      "  [104 104 104]\n",
      "  [ 34  34  34]]\n",
      "\n",
      " [[ 30  30  30]\n",
      "  [ 50  50  50]\n",
      "  [ 55  55  55]\n",
      "  ...\n",
      "  [175 175 175]\n",
      "  [113 113 113]\n",
      "  [ 36  36  36]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 31  31  31]\n",
      "  [ 28  28  28]\n",
      "  [ 29  29  29]\n",
      "  ...\n",
      "  [ 54  54  54]\n",
      "  [ 80  80  80]\n",
      "  [101 101 101]]\n",
      "\n",
      " [[ 31  31  31]\n",
      "  [ 29  29  29]\n",
      "  [ 30  30  30]\n",
      "  ...\n",
      "  [ 55  55  55]\n",
      "  [ 80  80  80]\n",
      "  [ 96  96  96]]\n",
      "\n",
      " [[ 29  29  29]\n",
      "  [ 31  31  31]\n",
      "  [ 28  28  28]\n",
      "  ...\n",
      "  [ 59  59  59]\n",
      "  [ 79  79  79]\n",
      "  [ 89  89  89]]]\n"
     ]
    }
   ],
   "source": [
    "cv2_image = cv2.imread(\"./FER-2013-5emotion/train/angry/Training_992349.jpg\")\n",
    "print(cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4ee0a7-9778-484a-99c8-032386edfb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26bc9de8d00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfc8f033-3279-4fee-a150-b4cd1c1d3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_data_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fcd7b3-bb11-4d33-88c5-80a57fe09e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95aa53a7-fc95-4fdf-a4b6-aa04f39c46be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19341 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    './FER-2013-5emotion/train',\n",
    "    subset='training',\n",
    "    target_size = (48, 48),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ac4a62-6a92-4f15-bc85-092a5378fed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4835 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = train_data_gen.flow_from_directory(\n",
    "    './FER-2013-5emotion/train',\n",
    "    subset='validation',\n",
    "    target_size = (48, 48),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25dd74ed-c15c-4223-b7a2-5e62ec6bb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6043 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_data_gen.flow_from_directory(\n",
    "    './FER-2013-5emotion/test',\n",
    "    target_size = (48, 48),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    color_mode = 'grayscale',\n",
    "    class_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fe1e029-1bf3-4704-96ae-0ecc4dab1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48, 48, 1), padding='same'))\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48, 48, 1)))\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "#model.add(Conv2D(512, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00b1a5f2-b6db-4abd-8de0-4fe871bf9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48, 48, 1), padding='same'))\n",
    "# model.add(Conv2D(64, kernel_size=(3,3)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(256, kernel_size=(3,3)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(512, kernel_size=(3,3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(512, kernel_size=(3,3), padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# # model.add(Dense(128, activation='relu'))\n",
    "# # model.add(Dropout(0.5))\n",
    "# model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45aa62d-c3e9-4ec7-8acc-33be9c086b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 44, 44, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 22, 22, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 22, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 20, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 18, 18, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 9, 9, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 9, 9, 256)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 7, 7, 512)         1180160   \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 3, 3, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              4719616   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 5125      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,292,741\n",
      "Trainable params: 6,292,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fe903de-8197-4b49-b9d4-4672ea1db527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001, decay=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c3c95f-bef6-4f9a-aa5e-325bfab926d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='c://Projects/emotion_detect_project/best_model_5emotion/', monitor='val_loss', mode='min', save_vest_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0a05127-7343-407a-be4c-434be83af13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "929d0a8d-b8c6-48fa-a2a5-d1240c6376dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_delta=0.0005, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56815377-9cb6-4176-9092-2f6322fe81fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 1.5757 - accuracy: 0.2966"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 19s 24ms/step - loss: 1.5758 - accuracy: 0.2963 - val_loss: 1.5670 - val_accuracy: 0.2997 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 1.4624 - accuracy: 0.3623"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 1.4617 - accuracy: 0.3628 - val_loss: 1.3429 - val_accuracy: 0.4373 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.2894 - accuracy: 0.4644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 1.2894 - accuracy: 0.4644 - val_loss: 1.1670 - val_accuracy: 0.5207 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 1.1460 - accuracy: 0.5296"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 1.1457 - accuracy: 0.5300 - val_loss: 1.0833 - val_accuracy: 0.5563 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 1.0614 - accuracy: 0.5725"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 1.0615 - accuracy: 0.5725 - val_loss: 0.9931 - val_accuracy: 0.5993 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.9990 - accuracy: 0.6046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.9990 - accuracy: 0.6046 - val_loss: 0.9569 - val_accuracy: 0.6165 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.9452 - accuracy: 0.6257"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.9450 - accuracy: 0.6259 - val_loss: 0.9346 - val_accuracy: 0.6219 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "603/604 [============================>.] - ETA: 0s - loss: 0.9050 - accuracy: 0.6496"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.9046 - accuracy: 0.6497 - val_loss: 0.9042 - val_accuracy: 0.6509 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.8702 - accuracy: 0.6595"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.8703 - accuracy: 0.6594 - val_loss: 0.8907 - val_accuracy: 0.6531 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.8340 - accuracy: 0.6770"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.8340 - accuracy: 0.6770 - val_loss: 0.8667 - val_accuracy: 0.6627 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.7955 - accuracy: 0.6858"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.7955 - accuracy: 0.6858 - val_loss: 0.8744 - val_accuracy: 0.6641 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.7611 - accuracy: 0.7045"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.7609 - accuracy: 0.7046 - val_loss: 0.8686 - val_accuracy: 0.6678 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "603/604 [============================>.] - ETA: 0s - loss: 0.7333 - accuracy: 0.7139"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.7336 - accuracy: 0.7138 - val_loss: 0.8639 - val_accuracy: 0.6701 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.7318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.6957 - accuracy: 0.7318 - val_loss: 0.8841 - val_accuracy: 0.6726 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "603/604 [============================>.] - ETA: 0s - loss: 0.6726 - accuracy: 0.7408"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.6724 - accuracy: 0.7409 - val_loss: 0.8678 - val_accuracy: 0.6722 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.6503 - accuracy: 0.7499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.6503 - accuracy: 0.7499 - val_loss: 0.8618 - val_accuracy: 0.6755 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "603/604 [============================>.] - ETA: 0s - loss: 0.6135 - accuracy: 0.7652"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.6135 - accuracy: 0.7652 - val_loss: 0.8644 - val_accuracy: 0.6819 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "601/604 [============================>.] - ETA: 0s - loss: 0.5894 - accuracy: 0.7756"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.5898 - accuracy: 0.7752 - val_loss: 0.8636 - val_accuracy: 0.6761 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.5578 - accuracy: 0.7854"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 13s 22ms/step - loss: 0.5583 - accuracy: 0.7852 - val_loss: 0.8994 - val_accuracy: 0.6809 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.5354 - accuracy: 0.7959"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.5352 - accuracy: 0.7959 - val_loss: 0.9316 - val_accuracy: 0.6823 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.5067 - accuracy: 0.8078"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.5071 - accuracy: 0.8078 - val_loss: 0.9172 - val_accuracy: 0.6807 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.4922 - accuracy: 0.8128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.4918 - accuracy: 0.8130 - val_loss: 0.9029 - val_accuracy: 0.6869 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.4746 - accuracy: 0.8208"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.4748 - accuracy: 0.8208 - val_loss: 0.9203 - val_accuracy: 0.6916 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.8330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.4462 - accuracy: 0.8330 - val_loss: 0.9423 - val_accuracy: 0.6885 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "601/604 [============================>.] - ETA: 0s - loss: 0.4253 - accuracy: 0.8425"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.4248 - accuracy: 0.8428 - val_loss: 0.9648 - val_accuracy: 0.6858 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "601/604 [============================>.] - ETA: 0s - loss: 0.4095 - accuracy: 0.8446"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.4088 - accuracy: 0.8449 - val_loss: 0.9774 - val_accuracy: 0.6881 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.8704"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.3501 - accuracy: 0.8704 - val_loss: 0.9946 - val_accuracy: 0.6908 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "603/604 [============================>.] - ETA: 0s - loss: 0.3363 - accuracy: 0.8731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 13s 22ms/step - loss: 0.3361 - accuracy: 0.8732 - val_loss: 0.9945 - val_accuracy: 0.6898 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.8774"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 13s 22ms/step - loss: 0.3290 - accuracy: 0.8774 - val_loss: 0.9987 - val_accuracy: 0.6921 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.3227 - accuracy: 0.8765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 13s 22ms/step - loss: 0.3227 - accuracy: 0.8765 - val_loss: 1.0258 - val_accuracy: 0.6918 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.8807"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 13s 22ms/step - loss: 0.3188 - accuracy: 0.8809 - val_loss: 1.0200 - val_accuracy: 0.6929 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.8851"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.3100 - accuracy: 0.8851 - val_loss: 1.0214 - val_accuracy: 0.6902 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "603/604 [============================>.] - ETA: 0s - loss: 0.3123 - accuracy: 0.8836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.3119 - accuracy: 0.8838 - val_loss: 1.0272 - val_accuracy: 0.6933 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "601/604 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.2976 - accuracy: 0.8863 - val_loss: 1.0453 - val_accuracy: 0.6921 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.8895"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.2979 - accuracy: 0.8895 - val_loss: 1.0521 - val_accuracy: 0.6947 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.8898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.2883 - accuracy: 0.8898 - val_loss: 1.0607 - val_accuracy: 0.6929 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.2966 - accuracy: 0.8893"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 13s 22ms/step - loss: 0.2966 - accuracy: 0.8893 - val_loss: 1.0558 - val_accuracy: 0.6931 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.8928"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.2872 - accuracy: 0.8928 - val_loss: 1.0577 - val_accuracy: 0.6945 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.2917 - accuracy: 0.8914"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.2917 - accuracy: 0.8914 - val_loss: 1.0541 - val_accuracy: 0.6954 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "601/604 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 13s 22ms/step - loss: 0.2976 - accuracy: 0.8886 - val_loss: 1.0493 - val_accuracy: 0.6964 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "601/604 [============================>.] - ETA: 0s - loss: 0.2961 - accuracy: 0.8854"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.2963 - accuracy: 0.8854 - val_loss: 1.0496 - val_accuracy: 0.6962 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "601/604 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8885"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 13s 22ms/step - loss: 0.2962 - accuracy: 0.8883 - val_loss: 1.0487 - val_accuracy: 0.6962 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.8912"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.2922 - accuracy: 0.8912 - val_loss: 1.0503 - val_accuracy: 0.6974 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "603/604 [============================>.] - ETA: 0s - loss: 0.2838 - accuracy: 0.8954"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.2839 - accuracy: 0.8954 - val_loss: 1.0513 - val_accuracy: 0.6968 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.2922 - accuracy: 0.8936"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 13s 22ms/step - loss: 0.2922 - accuracy: 0.8936 - val_loss: 1.0501 - val_accuracy: 0.6952 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "603/604 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.8886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.2920 - accuracy: 0.8887 - val_loss: 1.0549 - val_accuracy: 0.6950 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.8942"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.2871 - accuracy: 0.8940 - val_loss: 1.0560 - val_accuracy: 0.6952 - lr: 1.0000e-06\n",
      "Epoch 48/100\n",
      "601/604 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.8913"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.2875 - accuracy: 0.8913 - val_loss: 1.0524 - val_accuracy: 0.6952 - lr: 1.0000e-06\n",
      "Epoch 49/100\n",
      "603/604 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.8945"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.2862 - accuracy: 0.8945 - val_loss: 1.0556 - val_accuracy: 0.6954 - lr: 1.0000e-06\n",
      "Epoch 50/100\n",
      "601/604 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.8955"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.2857 - accuracy: 0.8954 - val_loss: 1.0549 - val_accuracy: 0.6954 - lr: 1.0000e-06\n",
      "Epoch 51/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.2882 - accuracy: 0.8913"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 22ms/step - loss: 0.2882 - accuracy: 0.8912 - val_loss: 1.0546 - val_accuracy: 0.6954 - lr: 1.0000e-06\n",
      "Epoch 52/100\n",
      "602/604 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8921"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.2933 - accuracy: 0.8922 - val_loss: 1.0550 - val_accuracy: 0.6954 - lr: 1.0000e-06\n",
      "Epoch 53/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.2855 - accuracy: 0.8936"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.2855 - accuracy: 0.8936 - val_loss: 1.0553 - val_accuracy: 0.6952 - lr: 1.0000e-06\n",
      "Epoch 54/100\n",
      "603/604 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.8915"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.2921 - accuracy: 0.8916 - val_loss: 1.0548 - val_accuracy: 0.6956 - lr: 1.0000e-06\n",
      "Epoch 55/100\n",
      "601/604 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.8930"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.2836 - accuracy: 0.8931 - val_loss: 1.0529 - val_accuracy: 0.6958 - lr: 1.0000e-06\n",
      "Epoch 56/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 0.2877 - accuracy: 0.8939"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: c://Projects/emotion_detect_project/best_model_5emotion\\/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - 14s 23ms/step - loss: 0.2877 - accuracy: 0.8939 - val_loss: 1.0550 - val_accuracy: 0.6958 - lr: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "emotion_model_info = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.n // BATCH_SIZE,\n",
    "    epochs = 100,\n",
    "    validation_data = valid_generator,\n",
    "    validation_steps = valid_generator.n // BATCH_SIZE,\n",
    "    callbacks=[checkpoint, earlystopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "055c69cf-82c1-4529-bfec-0a385d435c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 3s 15ms/step - loss: 0.8485 - accuracy: 0.6799\n",
      "accuracy: 67.99%\n",
      "loss: 0.85\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator, batch_size = BATCH_SIZE, steps= test_generator.n//BATCH_SIZE)\n",
    "print(f\"accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"loss: {test_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce5b1e69-eaed-4aa0-be4f-9f1876649dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 4s 33ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_generator)\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2e67940-ea1a-4bbb-b093-1c76f747d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bca9f737-36d0-4ce0-a32f-a279ff1c7bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.12      0.10      0.11       958\n",
      "     disgust       0.03      0.02      0.02       111\n",
      "        fear       0.14      0.12      0.13      1024\n",
      "       happy       0.25      0.31      0.28      1774\n",
      "     neutral       0.18      0.21      0.19      1233\n",
      "         sad       0.17      0.13      0.15      1247\n",
      "    surprise       0.12      0.12      0.12       831\n",
      "\n",
      "    accuracy                           0.18      7178\n",
      "   macro avg       0.14      0.14      0.14      7178\n",
      "weighted avg       0.17      0.18      0.17      7178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa75ca1-61c7-4d8b-bb34-0d9eae63e3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
